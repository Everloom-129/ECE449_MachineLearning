{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE449: Individaul Project\n",
    "Jie Wang\n",
    "\n",
    "Nov. 11, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "In this project, you need to solve an image classification task using coil-20-proc dataset.\n",
    "\n",
    "This dataset consists of 1,440 grayscale images of 20 objects (72 images per object). \n",
    "- Half of each category is for training and half for testing. (e.g. 36 images for training and another 36 images for testing).\n",
    "\n",
    "Through this project, you can learn how to customize dataset loading, design deep models, and train and test models. \n",
    "\n",
    "This is an individual project, so avoid copying others' code. Additionally, you need to print the accuracy results during the training process. \n",
    "\n",
    "> You should use a Python Jupyter Notebook to write down the process and accuracy results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The COIL-20 (Columbia Object Image Library) dataset\n",
    "a collection of grayscale images of 20 different objects. \n",
    "\n",
    "- The \"proc\" in \"COIL-20-proc\" indicates that the images in this dataset have been preprocessed. photographed on a motorized turntable against a black background to provide a 360-degree view. The objects are varied, including toys, household items, and office supplies, to provide a range of shapes and complexities.\n",
    "\n",
    "\n",
    "The preprocessing typically involves normalizing the images in terms of scale and orientation, and the background is often removed to isolate the object of interest. This makes the dataset particularly useful for computer vision tasks like object recognition and classification, where consistency in the input data can significantly improve the performance of machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Complete the custom dataset, and dataloader.\n",
    "Fill in the code to complete the custom dataset and dataloader, you can refer to https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class COIL20Dataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.images[index])\n",
    "        label = self.labels[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "# def load_coil20_dataset(directory, transform, test_split=0.2):\n",
    "#     # Get all the image file paths\n",
    "#     image_paths = glob.glob(f\"{directory}/*.png\") # Assuming the images are in PNG format\n",
    "#     labels = [int(path.split('_')[1]) for path in image_paths]  # Extracting labels from file names\n",
    "\n",
    "#     # Splitting the dataset into train and test\n",
    "#     split_idx = int(len(image_paths) * (1 - test_split))\n",
    "#     train_paths, test_paths = image_paths[:split_idx], image_paths[split_idx:]\n",
    "#     train_labels, test_labels = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "#     # Creating the dataset objects\n",
    "#     train_dataset = COIL20Dataset(train_paths, train_labels, transform)\n",
    "#     test_dataset = COIL20Dataset(test_paths, test_labels, transform)\n",
    "\n",
    "#     # Creating the dataloaders\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "#     return train_loader, test_loader\n",
    "\n",
    "# Example transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# the COIL-20 images are stored in a directory as below:\n",
    "# - coil-20-proc/\n",
    "#       - 01/obj1__0.png, obj1__1.png, ...\n",
    "#       - 02/...\n",
    "#       - ...\n",
    "#       - 20/....\n",
    "train_loader, test_loader = load_coil20_dataset('coil-20-proc/', transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
      "[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "[14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "[15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]\n",
      "[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]\n",
      "[17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17]\n",
      "[18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18]\n",
      "[19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n",
      "[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n"
     ]
    }
   ],
   "source": [
    "def load_coil20_dataset(base_directory, transform, test_split=0.2):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over each category directory to collect image paths and labels\n",
    "    for category in range(1, 21):  # Assuming 20 categories\n",
    "        directory = f\"{base_directory}/{str(category).zfill(2)}\"\n",
    "        category_image_paths = glob.glob(f\"{directory}/*.png\")\n",
    "        category_labels = [category] * len(category_image_paths)  # Labels are the category number\n",
    "        print(category_labels)\n",
    "        \n",
    "        image_paths.extend(category_image_paths)\n",
    "        labels.extend(category_labels)\n",
    "    \n",
    "    # Split the dataset into train and test\n",
    "    split_idx = int(len(image_paths) * (1 - test_split))\n",
    "    train_paths, test_paths = image_paths[:split_idx], image_paths[split_idx:]\n",
    "    train_labels, test_labels = labels[:split_idx], labels[split_idx:]\n",
    "    \n",
    "    # Creating the dataset objects\n",
    "    train_dataset = COIL20Dataset(train_paths, train_labels, transform)\n",
    "    test_dataset = COIL20Dataset(test_paths, test_labels, transform)\n",
    "    \n",
    "    # Creating the dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Example transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Assuming the base directory is 'coil-20-proc' containing subdirectories '01', '02', ..., '20'\n",
    "train_loader, test_loader = load_coil20_dataset('coil-20-proc', transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing a Neural Network\n",
    "Fill in the code to complete the custom model, you can refer to https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classify_NN(torch.nn.Module):\n",
    "    def __init__(self,alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear\n",
    "\n",
    "        )\n",
    "    def forward(self, x):\n",
    "    # you should complete forward function\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Customize some arguments\n",
    "Usually, this function is not used for jupyter notebook, as it violates the nature of interacive design.\n",
    "\n",
    "Still, I set it mannually for the argument input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_config():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # you should complete arguments\n",
    "    parser.add_argument()\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.device = torch.device(args.device)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train and Test the model.\n",
    "Fill in the code to complete the training and testing, and print the results and losses. You can refer to https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html and https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MyNet:\n",
    "    def __init__(self, args, logger):\n",
    "        self.args = args\n",
    "\n",
    "        # Operate the method\n",
    "        self.Mymodel = Classify_NN()\n",
    "        self.Mymodel.to(args.device)\n",
    "        self._print_args()\n",
    "\n",
    "    def _train(self, dataloader, criterion, optimizer):\n",
    "    # you should complete the training code, optimizer and loss function\n",
    "        \n",
    "        return train_loss / n_train, n_correct / n_train\n",
    "\n",
    "    def _test(self, dataloader, criterion):\n",
    "    # you should complete the testing code\n",
    "\n",
    "        return test_loss / n_test, n_correct / n_test\n",
    "\n",
    "    def run(self):\n",
    "    # you should complete run function for training and testing the model, and print the results and losses.\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = get_config()\n",
    "    net = MyNet(args)\n",
    "    net.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
