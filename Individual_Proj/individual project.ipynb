{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE449: Individaul Project\n",
    "Jie Wang\n",
    "\n",
    "Nov. 11, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "In this project, you need to solve an image classification task using coil-20-proc dataset.\n",
    "\n",
    "This dataset consists of 1,440 grayscale images of 20 objects (72 images per object). \n",
    "- Half of each category is for training and half for testing. (e.g. 36 images for training and another 36 images for testing).\n",
    "\n",
    "Through this project, you can learn how to customize dataset loading, design deep models, and train and test models. \n",
    "\n",
    "This is an individual project, so avoid copying others' code. Additionally, you need to print the accuracy results during the training process. \n",
    "\n",
    "> You should use a Python Jupyter Notebook to write down the process and accuracy results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The COIL-20 (Columbia Object Image Library) dataset\n",
    "a collection of grayscale images of 20 different objects. \n",
    "\n",
    "- The \"proc\" in \"COIL-20-proc\" indicates that the images in this dataset have been preprocessed. photographed on a motorized turntable against a black background to provide a 360-degree view. The objects are varied, including toys, household items, and office supplies, to provide a range of shapes and complexities.\n",
    "\n",
    "\n",
    "The preprocessing typically involves normalizing the images in terms of scale and orientation, and the background is often removed to isolate the object of interest. This makes the dataset particularly useful for computer vision tasks like object recognition and classification, where consistency in the input data can significantly improve the performance of machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Complete the custom dataset, and dataloader.\n",
    "Fill in the code to complete the custom dataset and dataloader, you can refer to https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class COIL20Dataset(Dataset):\n",
    "    def __init__(self, image_list, transform=None):\n",
    "        self.image_list = image_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.image_list[index]\n",
    "        # print(path)\n",
    "        image= Image.open(path)\n",
    "        # Extracting label from file names\n",
    "        label = int(os.path.basename(os.path.dirname(path))) -1 \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "# the COIL-20 images are stored in a directory as below:\n",
    "# - coil-20-proc/\n",
    "#       - 01/obj1__0.png, obj1__1.png, ...\n",
    "#       - 02/...\n",
    "#       - ...\n",
    "#       - 20/....\n",
    "root = 'coil-20-proc'\n",
    "\n",
    "ddd = COIL20Dataset(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_pair(root_dir):\n",
    "    image_list = []\n",
    "    \n",
    "    # Iterate over all directories in the root_dir\n",
    "    for label_dir in os.listdir(root_dir):\n",
    "        label_path = os.path.join(root_dir, label_dir)\n",
    "        # print(label_path)\n",
    "        if os.path.isdir(label_path):\n",
    "            # Extract label from the directory name\n",
    "            label = int(label_dir)  # start from 1 to 20\n",
    "            # Add each image to the list\n",
    "            for image_file in glob.glob(os.path.join(label_path, '*.png')):\n",
    "                image_list.append(image_file)\n",
    "    return image_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_coil20_dataset(base_directory='coil-20-proc', transform=None, test_split=0.2):\n",
    "    # Load the full dataset list\n",
    "    dataset_list = read_image_pair(base_directory)\n",
    "\n",
    "    # Splitting the dataset into train and test sets\n",
    "    dataset_size = len(dataset_list)\n",
    "    indices = list(range(dataset_size))\n",
    "\n",
    "    split = int(np.floor(test_split * dataset_size))\n",
    "    random.shuffle(indices)\n",
    "    train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # Split dataset into training and test sets\n",
    "    train_image_list = [dataset_list[i] for i in train_indices]\n",
    "    test_image_list = [dataset_list[i] for i in test_indices]\n",
    "    print(len(train_image_list))\n",
    "    # Creating datasets\n",
    "    train_dataset = COIL20Dataset(train_image_list, transform=transform)\n",
    "    test_dataset = COIL20Dataset(test_image_list, transform=transform)\n",
    "\n",
    "    # Creating data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "train_loader, test_loader = load_coil20_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.__len__() ) \n",
    "print(test_loader.__len__() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "def show_images_from_dataloader(dataloader):\n",
    "    # Get a batch of training data\n",
    "    images, labels = next(iter(dataloader))\n",
    "\n",
    "    # Make a grid from batch\n",
    "    out = torchvision.utils.make_grid(images)\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
    "    plt.title('Batch from dataloader')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Print labels\n",
    "    print('Labels:', labels.numpy())\n",
    "\n",
    "# Example usage with your train_loader\n",
    "# show_images_from_dataloader(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing a Neural Network\n",
    "Fill in the code to complete the custom model, you can refer to https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classify_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(128*128, 512),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 20)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Customize some arguments\n",
    "Usually, this function is not used for jupyter notebook, as it violates the nature of interacive design.\n",
    "\n",
    "Still, I set it mannually for the argument input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train and Test the model.\n",
    "Fill in the code to complete the training and testing, and print the results and losses. You can refer to https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html and https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coil20_model:\n",
    "    def __init__(self, train_loader, test_loader):\n",
    "        self.train_loader = train_loader \n",
    "        self.test_loader = test_loader\n",
    "        self.Mymodel = Classify_NN()\n",
    "        self.criterion = nn.CrossEntropyLoss()  \n",
    "        self.optimizer = torch.optim.Adam(self.Mymodel.parameters(), lr=0.001) \n",
    "\n",
    "    def _train(self, dataloader):\n",
    "        train_loss = 0\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        self.Mymodel.train()\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.float(), labels.long()\n",
    "            outputs = self.Mymodel(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = train_loss / len(dataloader)\n",
    "        accuracy = n_correct / n_samples\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def _test(self, dataloader):\n",
    "        test_loss = 0\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        self.Mymodel.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in dataloader:\n",
    "                images, labels = images.float(), labels.long()\n",
    "                outputs = self.Mymodel(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                n_samples += labels.size(0)\n",
    "                n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = test_loss / len(dataloader)\n",
    "        accuracy = n_correct / n_samples\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def run(self, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_accuracy = self._train(self.train_loader)\n",
    "            print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}')\n",
    "            print(\"start testing\")\n",
    "            test_loss, test_accuracy = self._test(self.test_loader)\n",
    "            print(f'Epoch {epoch+1}: Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def _visualize(self, accuracy, val_accuracy):\n",
    "    #     plt.plot(accuracy, label='accuracy')\n",
    "    #     plt.plot(val_accuracy, label='val_accuracy')\n",
    "    #     plt.xlabel('Epoch')\n",
    "    #     plt.ylabel('Accuracy')\n",
    "    #     plt.ylim([0, 1])\n",
    "    #     plt.legend(loc='lower right')\n",
    "    #     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n",
      "Epoch 1: Train Loss: 1.5518, Accuracy: 0.54\n",
      "Epoch 1: Test Loss: 0.6255, Accuracy: 0.83\n",
      "Epoch 2: Train Loss: 0.3411, Accuracy: 0.90\n",
      "Epoch 2: Test Loss: 0.2276, Accuracy: 0.95\n",
      "Epoch 3: Train Loss: 0.1035, Accuracy: 0.98\n",
      "Epoch 3: Test Loss: 0.0722, Accuracy: 0.98\n",
      "Epoch 4: Train Loss: 0.0386, Accuracy: 1.00\n",
      "Epoch 4: Test Loss: 0.0634, Accuracy: 0.99\n",
      "Epoch 5: Train Loss: 0.0534, Accuracy: 0.99\n",
      "Epoch 5: Test Loss: 0.0411, Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    " transforms.Resize((128, 128)),\n",
    " transforms.ToTensor(),\n",
    " ])\n",
    "train,test = load_coil20_dataset('coil-20-proc/',transform)\n",
    "ece449_model = Coil20_model(train,test )  \n",
    "ece449_model.run(5)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "flatten",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\# Courses\\## 大四\\FA23\\ECE449_MP\\Individual_Proj\\individual project.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m predicted\u001b[39m.\u001b[39mitem()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(image_path)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m predicted_label \u001b[39m=\u001b[39m evaluate_single_image(ece449_model\u001b[39m.\u001b[39;49mMymodel, image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredicted Label: \u001b[39m\u001b[39m{\u001b[39;00mpredicted_label\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\# Courses\\## 大四\\FA23\\ECE449_MP\\Individual_Proj\\individual project.ipynb Cell 19\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39meval()  \u001b[39m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(image)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m predicted\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\# Courses\\## 大四\\FA23\\ECE449_MP\\Individual_Proj\\individual project.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_relu_stack(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%23%20Courses/%23%23%20%E5%A4%A7%E5%9B%9B/FA23/ECE449_MP/Individual_Proj/individual%20project.ipynb#X35sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\flatten.py:46\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mflatten(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend_dim)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:529\u001b[0m, in \u001b[0;36mImage.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    527\u001b[0m     deprecate(\u001b[39m\"\u001b[39m\u001b[39mImage categories\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mis_animated\u001b[39m\u001b[39m\"\u001b[39m, plural\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    528\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_category\n\u001b[1;32m--> 529\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: flatten"
     ]
    }
   ],
   "source": [
    "image_path = 'coil-20-proc/04/obj4__66.png'\n",
    "def evaluate_single_image(model, image):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted.item()\n",
    "\n",
    "image = Image.open(image_path)\n",
    "\n",
    "predicted_label = evaluate_single_image(ece449_model.Mymodel, image)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the model\n",
    "torch.save(ece449_model.Mymodel.state_dict(), 'coil20_model.pth')\n",
    "\n",
    "# To load the model\n",
    "ece449_model.Mymodel.load_state_dict(torch.load('coil20_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
