{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE449 Assignment 2\n",
    "- Jie Wang \n",
    "- 3200112404\n",
    "- Nov.2, 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Design a Linear layer using nn. Linear (). \n",
    "`nn.Linear()` is used to implement fully connected layer functions. \n",
    "\n",
    "The feature is in `B×D`  format. B denotes batch_size, D denotes the dimension of expected features in the input. Input feature is `32×512`. Please design the corresponding layer to output a tensor with 32×256 dimension and print the dimension of the output. \n",
    "\n",
    "Fill in the blanks below to meet the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.randn(32, 512) \n",
    "m = nn.Linear(512,256) \n",
    "output = m(input_tensor)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Design a Convolutional Neural Networks using nn.Conv2d(). \n",
    "nn.Conv2d  is  used  to  create  two-dimensional  convolutional  layers,  typically  employed  in \n",
    "Convolutional Neural Networks (CNNs). The feature is in B×C×H×W format. B denotes batch_size, \n",
    "C denotes channel, H denotes height, W denotes width. Input feature is 32×36×512×256. \n",
    " \n",
    "(a) You use conv with 3×3 kernel size. Stride is 2. Please design the corresponding layer to output \n",
    "a tensor with 32×34×255×127 dimension and print the dimension of the output. Fill in the \n",
    "blanks below to meet the requirements. \n",
    ">>> input = torch.randn(32, 36, 512, 256) \n",
    "\n",
    ">>> m = nn.Conv2d() \n",
    "\n",
    ">>> output = m(input) \n",
    " \n",
    "(b) You  use  conv  with  2×4  kernel  size.  Strides  are  1  and  2  respectively.  Padding  is  3  and  1 \n",
    "respectively. Please design the corresponding layer to output a tensor with 32×35×517×128 \n",
    "dimension  and  print  the  dimension  of  the  output.  Fill  in  the  blanks  below  to  meet  the \n",
    "requirements. \n",
    ">>> m = nn.Conv2d(______________________________________) \n",
    "\n",
    ">>> output = m(input) \n",
    " \n",
    "(c) You  use  conv  with  3×5  kernel  size.  Strides  are  3  and  2  respectively.  Padding  is  3  and  1 \n",
    "respectively. Dilations are 2 and 1 respectively. Group is 2. Input feature is 32×16×512×256. \n",
    "Please design the corresponding layer to output a tensor with 32×32×172×127 dimension and \n",
    "print the dimension of the output. Fill in the blanks below to meet the requirements. \n",
    ">>> input = torch.randn(32, 16, 512, 256) \n",
    "\n",
    ">>> m = nn.Conv2d(_______________________________________) \n",
    "\n",
    ">>> output = m(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "where:\n",
    "- `in_channels`: Number of channels in the input image\n",
    "- `out_channels`: Number of channels produced by the convolution\n",
    "- `kernel_size`: Size of the convolving kernel\n",
    "- `stride`: Stride of the convolution. Default: 1\n",
    "- `padding`: Padding added to all four sides of the input. Default: 0\n",
    "- `dilation`: Spacing between kernel elements. Default: 1\n",
    "- `groups`: Number of blocked connections from input channels to output channels. Default: 1\n",
    "- `bias`: If `True`, adds a learnable bias to the output. Default: `True`\n",
    "- `padding_mode`: Accepted values are 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In these examples, we've defined convolutional layers with various parameters to transform the input tensor to the desired output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 34, 255, 127])\n",
      "torch.Size([32, 35, 517, 128])\n",
      "torch.Size([32, 32, 172, 127])\n"
     ]
    }
   ],
   "source": [
    "### (a)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.randn(32, 36, 512, 256)\n",
    "m = nn.Conv2d(36, 34, 3, stride=2)\n",
    "output = m(input_tensor)\n",
    "print(output.shape)  \n",
    "\n",
    "### (b)\n",
    "m = nn.Conv2d(36, 35, (2, 4), stride=(1, 2), padding=(3, 1))\n",
    "output = m(input_tensor)\n",
    "print(output.shape)  \n",
    "\n",
    "###(c)\n",
    "input_tensor = torch.randn(32, 16, 512, 256)\n",
    "m = nn.Conv2d(16, 32, (3, 5), stride=(3, 2), padding=(3, 1), dilation=(2, 1), groups=2)\n",
    "output = m(input_tensor)\n",
    "print(output.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Design a RNN using nn.RNN(). \n",
    "nn.RNN() is used to create one layer of a Recurrent Neural Network (RNN) model. The feature is \n",
    "in L×B×D format. L denotes sequence length, B denotes batch_size, D denotes the dimension of \n",
    "expected features in the input. Input feature is 64×8×32. You want to design a 2 layers RNN. The \n",
    "hidden size is 64. Please design the corresponding layer to output a tensor with 64×8×64 dimension \n",
    "and print the dimension of the output. Fill in the blanks below to meet the requirements. \n",
    " \n",
    ">>> input = torch.randn(64, 8, 32) \n",
    "\n",
    ">>> rnn = nn.RNN(____________) \n",
    "\n",
    ">>> h0 = torch.randn(___________) \n",
    "\n",
    ">>> output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_tensor = torch.randn(64, 8, 32)\n",
    "rnn = nn.RNN(input_size=32, hidden_size=64, num_layers=2)\n",
    "h0 = torch.randn(2, 8, 64)\n",
    "output, hn = rnn(input_tensor, h0)\n",
    "print(output.shape)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Design a TransformerEncoderLayer using nn.TransformerEncoderLayer().   \n",
    "`nn.TransformerEncoderLayer()` is used to create the encoding layers of a Transformer model, which \n",
    "includes  multi-head  self-attention  layers,  feed-forward  neural  network  layers  and  layer \n",
    "normalization. \n",
    "\n",
    "The feature is in B×L×D format. B denotes batch_size, L denotes sequence length, \n",
    "D denotes the dimension of expected features in the input. Input feature is 128×16×512. The head \n",
    "number  of  Transformer  is  8.  \n",
    "\n",
    "\n",
    "Please  design  the  corresponding  layer  to  output  a  tensor  with \n",
    "128×16×512 dimension and print the dimension of the output. Set batch_first is True. Fill in the \n",
    "blanks below to meet the requirements. \n",
    " \n",
    ">>> input = torch.rand(128, 16, 512) \n",
    "\n",
    ">>> encoder_layer = nn.TransformerEncoderLayer(_____________) \n",
    "\n",
    ">>> out = encoder_layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_tensor = torch.rand(128, 16, 512)\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "output = encoder_layer(input_tensor)\n",
    "print(output.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Calculate  the  number  of  parameters  of  depthwise  and  `1×1`  convolution. Assume  the  feature dimension is in `H×W×C` format.   \n",
    "（1）Input feature is `16×16×32`. We use depthwise conv with `3×3` kernel size. Calculate the number of parameters in the kernel. Assume there is no bias term.   \n",
    "（2）Input feature is `24×24×64`. We use 64 `1×1` conv kernels. Calculate the number of parameters \n",
    "in the kernel. Assume there is no bias term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depthwise Convolution\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Number of Parameters} = \\text{Kernel Height} \\times \\text{Kernel Width} \\times \\text{Input Channels}\n",
    "$$\n",
    "\n",
    "#### Example (1)\n",
    "- Input feature dimensions: $ H \\times W \\times C = 16 \\times 16 \\times 32 $\n",
    "- Kernel size: $ 3 \\times 3 $\n",
    "\n",
    "$$\n",
    "\\text{Number of Parameters} = 3 \\times 3 \\times 32 = 288\n",
    "$$\n",
    "\n",
    "### $1 \\times 1$ Convolution\n",
    "$1 \\times 1$ convolution is a special case of convolution where the kernel size is $1 \\times 1$. It is often used for dimensionality reduction, increasing non-linearity, and learning channel-wise features.\n",
    "\n",
    "The formula to calculate the number of parameters in a $1 \\times 1$ convolutional layer is:\n",
    "\n",
    "$$\n",
    "\\text{Number of Parameters} = \\text{Kernel Height} \\times \\text{Kernel Width} \\times \\text{Input Channels} \\times \\text{Output Channels}\n",
    "$$\n",
    "\n",
    "#### Example (2)\n",
    "- Input feature dimensions: $ H \\times W \\times C = 24 \\times 24 \\times 64 $\n",
    "- Number of $1 \\times 1$ conv kernels (output channels): 64\n",
    "\n",
    "$$\n",
    "\\text{Number of Parameters} = 1 \\times 1 \\times 64 \\times 64 = 4096\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
